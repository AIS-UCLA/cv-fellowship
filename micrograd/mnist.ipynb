{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST with micrograd\n",
    "In this notebook, you will implement a 2-layer (784-800-10) fully connected\n",
    "feed-forward neural network for MNIST classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import csv\n",
    "import micrograd.nn as nn\n",
    "from micrograd.engine import Value\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's setup our dataset. Keras automatically splits the MNIST data into\n",
    "train and test segments for us.\n",
    "\n",
    "The \"x\" variables are the images, while the \"y\" variables are the ground truths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_x, train_y), (test_x, test_y) = mnist.load_data()\n",
    "print(f\"{train_x.shape=}\\n{train_y.shape=}\\n{test_x.shape=}\\n{test_y.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize one of the data points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(train_x[0], cmap=\"gray\")\n",
    "plt.title(f\"Ground truth={train_y[0]}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our data loaded, we can initialize our model, using the\n",
    "abstractions we wrote in `micrograd/nn.py` (imported above as `nn`).\n",
    "\n",
    "Remember that we are looking to create a multi-layer perceptron, with one hidden\n",
    "layer of dimension 800, an input layer of 784, and output layer of 10.\n",
    "\n",
    "Since our input dimension is now 784, we need to remember to reshape the input\n",
    "images (and normalize)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.MLP(784, [800, 10])\n",
    "\n",
    "train_x = train_x.reshape(-1, 784) / 255\n",
    "test_x = test_x.reshape(-1, 784) / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy your implementation of softmax from step two:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z: list[Value], C:int=10) -> list[Value]:\n",
    "  pass # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can try evaluating the model (with random weights)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax(model(train_x[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, our engine is way to slow to train this much larger model. As such,\n",
    "pre-trained weights have been provided in the weights.csv file. The code below\n",
    "loads these weights into your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(model.layers[0].parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"weights.csv\", \"r\") as f:\n",
    "  reader = csv.reader(f)\n",
    "  weights = list(reader)\n",
    "\n",
    "for i, p in enumerate(model.parameters()):\n",
    "  p.data = float(weights[i][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our model is trained, let's try running it on some test examples.\n",
    "Try changing `SAMPLE_IDX` to see different examples.\n",
    "\n",
    "NOTE: This may be quite slow, depending on your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_IDX = 0\n",
    "\n",
    "pred = softmax(model(test_x[SAMPLE_IDX]))\n",
    "pred_idx = pred.index(max(pred, key=lambda x: x.data))\n",
    "plt.imshow(test_x[SAMPLE_IDX].reshape(28,28), cmap=\"gray\")\n",
    "plt.title(f\"Ground truth={test_y[SAMPLE_IDX]} Prediction={pred_idx}\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv-fellowship",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
